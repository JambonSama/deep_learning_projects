\section{Discussions} 

First, all five networks perform well, with average accuracies ranging from 83\% to 91\%.

In terms of accuracy, the networks are sorted as naive net, WS net, AL net and the best is WS+AL net (DCM is not sorted, because it doesn't classify the same thing, so it's not compared). 
This is what was expected, though AL nets (both AL net and WS+AL) depend on the weighting of the auxiliary losses. 

In terms of optimal number of training epochs, the networks are sorted as WS+AL net, AL net, naive net and the best is WS net. 
Indeed, a lower optimal number of training epochs indicates that the networks converge faster to an optimum between underfitting and overfitting.

The number of parameters is roughly doubled from WS networks to those without (again, not considering DCM), which is expected as well.

The epoch training durations seem like reasonable times to spend on a epoch.

Statistically, if their is a probability of 95.2\% of correctly classifying a digit (DCM), then, the probability of correctly classifying two digits is $95.2^2 = 90.6$\%, which is the accuracy of the WS+AL net; so those results seem consistent with each other.

The loss graphs on Figure~\ref{fig::losses} show that losses without auxiliary losses are initially lower than with AL. It also show that the losses with AL evolve similarly, and the losses without also evolve similarly.
They also show that in case of auxiliary losses as implemented here, all the intermediate losses present a similar value and evolution.

On a less positive note, the DCM sometimes present very average accuracy (going as low as 36\% on one run), with very high variances (46\% for the same run). It happens scarcely, but it still happens, and the reason why is not well understood. 

All in all, for a qualitative assessment, the WS+AL net is a very good one, and classifies well the pairs as required in the project guidelines.

As for the running time of the script, 20 to 30 minutes for $2\cdot20\cdot5$ networks trainings seems reasonable enough.
