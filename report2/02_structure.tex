\section{Structure}
To program our neural network, we decide to separate the program in two main \mintinline{python}{class} as shown in the Figure XXXX

INSERT FIGURE

\subsection{Class neural network}

The first \mintinline{python}{class} is \mintinline{python}{NeuralNetwork}. This \mintinline{python}{class} is use to initialize the neural network and train it, it contain the following function (The complete code for \mintinline{python}{class} is in the appendix~\ref{app::NeuralNetwork}):

\begin{itemize}
	\item \mintinline{python}{def __init__(self)} : construct a empty neural network.
	\item \mintinline{python}{def sequential(self,*args)} : function to define the layer of the neural network, it need a series of constructor like Linear.
	\item \mintinline{python}{def run(self, input_data)} : Compute the result of the neural network in function of the input.
	\item \mintinline{python}{def loss(self, x, target)} : Compute the MSE loss.
	\item \mintinline{python}{def d_loss(self, x, target)} : Compute the derivative of the loss.
	\item \mintinline{python}{def test_error(self, test_input, test_label)} : compute the number of error of classification in function of a test set input and these label.
	\item \mintinline{python}{def params(self)} : Print all information of the network like layer type, number of input, number of output, weight and bias matrix for each layer.
	\item \mintinline{python}{def train_network(self, train_set, train_output, epochs, batch_size, learning_rate, print_error=False,}\\
	\mintinline{python}{test_set=None, test_label=None)} : Train the neural network with or without printing the percentage of error at each epochs in function of the boolean  \mintinline{python}{print_error}
\end{itemize}

To make the \mintinline{python}{class NeuralNetwork} work, we need a class of the layer.
 
\subsection{Layer}

The second \mintinline{python}{class} is \mintinline{python}{Linear}. This \mintinline{python}{class} ..... The \mintinline{python}{class Linear} contain the following function (The complete code for \mintinline{python}{class Linear} is in the appendix~\ref{app::Layer}) 

\begin{itemize}
	\item \mintinline{python}{def __init__(self, nb_input, nb_output)} : Create the Linear layer and initialize the weight and bias matrix.
	\item \mintinline{python}{def activation(self,x)} : Compute the activation function, in a linear layer, there is no activation function thus the function return the input.
	\item \mintinline{python}{def d_activation(self, x)} : Compute the derivative of the activation function, in a linear layer, there is no activation function thus the derivative activation return 1.
	\item \mintinline{python}{def forward_pass(self,x)} : Compute the forward pass for the layer, multiply the input by the weight and add the bias then call the activation function with the result. Return the result of the activation function. Save the input and the result of z to facilitate the computation of the backward pass
	\item \mintinline{python}{def backward_pass(self, dl_dy)} Compute the backward pass for the layer, compute the derivative of the loss in function of the weight, bias and input. Then accumulate the derivative of the weight and bias.
	\item \mintinline{python}{def update_parameters(self, learning_rate, batch_size)} : Update the parameters in function of the accumulative derivative of the weight and the bias with a learning rate.
	\item \mintinline{python}{def params(self)} : Print useful information about the current layer, his type, number of neuron input and output, shape of the weight and bias matrix and value of this matrix.
\end{itemize}

To implement the ReLu and the TanH layer, two child class was made. In this two class, the constructor, the activation and the derivative activation function are overload to have the correct function and initialization for ReLu and TanH layer. 