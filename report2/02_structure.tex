\section{Structure}
To program the neural network, the program is separated in two major parts. The first part is the \mintinline{python}{class NeuralNetwork} which encapsulates a neural network per instance. The second part is the layer which is a series of classes to make the Linear, ReLu and TanH layers and compute theirs forward and backward passes. 

\subsection{Class neural network}

The first class is \mintinline{python}{NeuralNetwork}. This class is used to initialize the neural network and train it.
It contains the following functions (the complete code for this class is available in the Appendix~\ref{app::NeuralNetwork}):

\begin{itemize}
	\item \mintinline{python}{def __init__(self)} : Constructs an empty neural network.
	\item \mintinline{python}{def sequential(self, *args)} : Defines the layer of the neural network, it requires a non-fixed non-null number of arguments, amongst Linear, ReLu or TanH.
	\item \mintinline{python}{def run(self, input_data)} : Computes the result of the neural network as a function of the input.
	\item \mintinline{python}{def loss(self, x, target)} : Computes the MSE loss.
	\item \mintinline{python}{def d_loss(self, x, target)} : Computes the derivative of the loss.
	\item \mintinline{python}{def test_error(self, test_input, test_label)} : Computes the number of classification errors as a function of a test set and test label, both passed as arguments.
	\item \mintinline{python}{def params(self)} : Prints all information of the network like layer type, number of inputs, number of outputs, weight and bias matrices for each layer.
	\item \mintinline{python}{def train_network(self, train_set, train_output, epochs, batch_size, learning_rate, print_error=False,}\\
	\mintinline{python}{test_set=None, test_label=None)} : Trains the neural network with a training set, a training output, a number of epochs, a batch size, a learning rate and a boolean for printing the percentage of error on a test set at each epochs. If that boolean is set to True, a test set and its label set are required. For a stochastic gradient descent the batch size equal to 1.
\end{itemize}

To use the \mintinline{python}{sequential} function of the \mintinline{python}{class NeuralNetwork}, it is required that there at least be one layer.
 
\subsection{Layer}

The second class is \mintinline{python}{Linear}. This class contains the following functions (the complete code for this class is available in the Appendix~\ref{app::Layer}) 

\begin{itemize}
	\item \mintinline{python}{def __init__(self, nb_input, nb_output)} : Creates the Linear layer and initializes the weight and bias matrix with the Xavier initialization.
	\item \mintinline{python}{def activation(self, x)} : Computes the activation function. In a linear layer, there is no activation function thus the function returns the input.
	\item \mintinline{python}{def d_activation(self, x)} : Computes the derivative of the activation function. In a linear layer, there is no activation function thus the activation derivative returns 1.
	\item \mintinline{python}{def forward_pass(self, x)} : Computes the forward pass for the layer. It multiplies the input by the weight and add the bias. Then, it calls the activation function with the result. It returns the result of the activation function. And it also saves the input and the activation function input to facilitate the computation of the backward pass.
	\item \mintinline{python}{def backward_pass(self, dl_dy)} Computes the backward pass for the layer. It computes the derivative of the loss with respect to the weight, bias and input. Then it accumulates the derivative of the weight and bias.
	\item \mintinline{python}{def update_parameters(self, learning_rate, batch_size)} : Updates the parameters in function of the cumulative derivative of the weight and the bias with a learning rate and the batch size.
	\item \mintinline{python}{def params(self)} : Prints useful information about the layer, its type, number of input and output neurons, shape of the weight and bias matrix, and value of those matrix.
\end{itemize}

To implement the ReLu and the TanH layer, two derived classes were made. 
In these two classes, the constructor, the activation and the derivative activation function are overload to have the correct functions and initializations.% for ReLu and TanH layer. 
These functions are summarized in the Table~\ref{tab::layer_recap} below:

\begin{table}[H]
	\sffamily
	\footnotesize
	\arrayrulecolor{white}
	\arrayrulewidth=1pt
	\renewcommand{\arraystretch}{1.5}
	\rowcolors[\hline]{1}{.!50!White}{}
	\centering
	\begin{tabular}{@{} A|A|A|A @{}}
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries  Layer &
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries Initialization &
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries Activation function &
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries Derivative Activation function  \\   
		\arraycolor{Black}
		ReLu			& Xavier	& max(0,x)		& 1 if x>0 else 0 		 		\\
		TanH			& He		& tanh(x) 		& 1 - tanh$^2$(x)		
	\end{tabular}
	\caption{ReLu and TanH descriptions.}
	\label{tab::layer_recap}
\end{table}