\section{Structure}
To program the neural network, the program is separate in two main part. The first part is the \mintinline{python}{class NeuralNetwork} which run the neural network. The second part is the layer which is a series of class to make the Linear, ReLu and TanH layers and compute theirs forward and backward pass. 

\subsection{Class neural network}

The first class is \mintinline{python}{NeuralNetwork}. This class is use to initialize the neural network and train it, it contain the following function (The complete code for this class is in the appendix~\ref{app::NeuralNetwork}):

\begin{itemize}
	\item \mintinline{python}{def __init__(self)} : construct a empty neural network.
	\item \mintinline{python}{def sequential(self,*args)} : function to define the layer of the neural network, it need a series of constructor : Linear, ReLu or TanH.
	\item \mintinline{python}{def run(self, input_data)} : Compute the result of the neural network in function of the input.
	\item \mintinline{python}{def loss(self, x, target)} : Compute the MSE loss.
	\item \mintinline{python}{def d_loss(self, x, target)} : Compute the derivative of the loss.
	\item \mintinline{python}{def test_error(self, test_input, test_label)} : Compute the number of error of classification in function of a test set input and test label.
	\item \mintinline{python}{def params(self)} : Print all information of the network like layer type, number of input, number of output, weight and bias matrix for each layer.
	\item \mintinline{python}{def train_network(self, train_set, train_output, epochs, batch_size, learning_rate, print_error=False,}\\
	\mintinline{python}{test_set=None, test_label=None)} : Train the neural network with a training set, a training output, a number of epochs, a batch size (For a stochastic gradient descent the batch size equal to 1), a learning rate and a boolean for printing the percentage of error on a test set at each epochs, if it set to True a test set and his label are needed.
\end{itemize}

To use the \mintinline{python}{class NeuralNetwork}, a class for the layer is needed for the use of the sequential function.
 
\subsection{Layer}

The second class is Linear. The \mintinline{python}{class Linear} contain the following function (The complete code for \mintinline{python}{class Linear} is in the appendix~\ref{app::Layer}) 

\begin{itemize}
	\item \mintinline{python}{def __init__(self, nb_input, nb_output)} : Create the Linear layer and initialize the weight and bias matrix with the Xavier initialization.
	\item \mintinline{python}{def activation(self,x)} : Compute the activation function, in a linear layer, there is no activation function thus the function return the input.
	\item \mintinline{python}{def d_activation(self, x)} : Compute the derivative of the activation function, in a linear layer, there is no activation function thus the derivative activation return 1.
	\item \mintinline{python}{def forward_pass(self,x)} : Compute the forward pass for the layer, multiply the input by the weight and add the bias then call the activation function with the result. Return the result of the activation function. Save the input and the result of z to facilitate the computation of the backward pass.
	\item \mintinline{python}{def backward_pass(self, dl_dy)} Compute the backward pass for the layer, compute the derivative of the loss with respect of the weight, bias and input. Then accumulate the derivative of the weight and bias.
	\item \mintinline{python}{def update_parameters(self, learning_rate, batch_size)} : Update the parameters in function of the accumulative derivative of the weight and the bias with a learning rate and the batch size.
	\item \mintinline{python}{def params(self)} : Print useful information about the  layer, his type, number of neuron input and output, shape of the weight and bias matrix and value of those matrix.
\end{itemize}

To implement the ReLu and the TanH layer, two child class was made. In this two class, the constructor, the activation and the derivative activation function are overload to have the correct function and initialization for ReLu and TanH layer. Those function are resume in the Table~\ref{tab::layer_recap}

\begin{table}[H]
	\sffamily
	\footnotesize
	\arrayrulecolor{white}
	\arrayrulewidth=1pt
	\renewcommand{\arraystretch}{1.5}
	\rowcolors[\hline]{1}{.!50!White}{}
	\centering
	\begin{tabular}{@{} A|A|A|A @{}}
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries  Layer &
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries Initialization &
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries Activation function &
		\cellcolor{ForestGreen}\arraycolor{White}\bfseries Derivative Activation function  \\   
		\arraycolor{Black}
		ReLu			& Xavier	& max(0,x)		& 1 if x>0 else 0 		 		\\
		TanH			& He		& tanh(x) 		& 1 - tanh$^2$(x)		
	\end{tabular}
	\caption{ReLu and TanH description.}
	\label{tab::layer_recap}
\end{table}